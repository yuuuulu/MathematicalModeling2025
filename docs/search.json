[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mathematical Modeling 2025",
    "section": "",
    "text": "Thanks to our efforts on the New Year 2025’s Eve\n\nResults are not important in a competition, but the process of getting there is.\nReflections are important, and we should always reflect on our work."
  },
  {
    "objectID": "MCM2025code.html",
    "href": "MCM2025code.html",
    "title": "The Riddle of Olympic Medal Table: Mathematical Model Prediction and Multi-factor Analysis",
    "section": "",
    "text": "The Summer Olympics are held every four years. The Olympic medal tally and its related influencing factors are a focus of general concern. Therefore, we set up a mathematical model to analyze and forecast the relevant data of the Olympic Games.\nFirstly, we build a combinatorial to predict the 2028 Olympic medal. This model comprehensively analyzes the influence of different factors and time series on the Olympic medal table. Therefore, we predict medal distribution that in 2028. Then, we combined the to calculate the number of gold medals in the predicted medals and evaluated the analysis through the posterior distribution. The , indicating that the model has good prediction performance.\nSecondly, we used the data to divide countries into two clusters (potential and non-potential) based on historical medal count and whether they have won medals. Then, we use clustering method for analysis and prediction. The final prediction is that will win their first medals, and then we quantify the probability distribution and predict their odds.\nThirdly, we set up a to analyze and explore the impact and relationship between events and organizers on the number of medals. The model training results show that the event has a significant impact on the number of medals, and the . The host country also had a significant on the number of medals, with a . In addition, we established Lasso regression model to explore the most important sports in different countries, and the evaluation error of the model was small, indicating that the model was highly interpretable.\nFourthly, we still use to analyze the influence of coach factor is used to explore the effect of “great coach”. The model results show that the model’s fitting degree . In addition, the model indicates that the great coach has a greater contribution to the number of medals. Then, we use algorithm to solve a model, which employs a to control variables, showing the United States, Japan, Belarus three countries to hire the program and its possible impact on the number of medals in the next Olympic Games.\nFinally, we explored three insights from our model and explain their reference value to country Olympic Committees.\n\nKeywords: Hierarchical Bayesian, Dirichlet Distribution, K-Means++, MLR, Penalized Lasso Regression, Dynamic Penalized Function, SA"
  },
  {
    "objectID": "MCM2025code.html#abstract",
    "href": "MCM2025code.html#abstract",
    "title": "The Riddle of Olympic Medal Table: Mathematical Model Prediction and Multi-factor Analysis",
    "section": "",
    "text": "The Summer Olympics are held every four years. The Olympic medal tally and its related influencing factors are a focus of general concern. Therefore, we set up a mathematical model to analyze and forecast the relevant data of the Olympic Games.\nFirstly, we build a combinatorial to predict the 2028 Olympic medal. This model comprehensively analyzes the influence of different factors and time series on the Olympic medal table. Therefore, we predict medal distribution that in 2028. Then, we combined the to calculate the number of gold medals in the predicted medals and evaluated the analysis through the posterior distribution. The , indicating that the model has good prediction performance.\nSecondly, we used the data to divide countries into two clusters (potential and non-potential) based on historical medal count and whether they have won medals. Then, we use clustering method for analysis and prediction. The final prediction is that will win their first medals, and then we quantify the probability distribution and predict their odds.\nThirdly, we set up a to analyze and explore the impact and relationship between events and organizers on the number of medals. The model training results show that the event has a significant impact on the number of medals, and the . The host country also had a significant on the number of medals, with a . In addition, we established Lasso regression model to explore the most important sports in different countries, and the evaluation error of the model was small, indicating that the model was highly interpretable.\nFourthly, we still use to analyze the influence of coach factor is used to explore the effect of “great coach”. The model results show that the model’s fitting degree . In addition, the model indicates that the great coach has a greater contribution to the number of medals. Then, we use algorithm to solve a model, which employs a to control variables, showing the United States, Japan, Belarus three countries to hire the program and its possible impact on the number of medals in the next Olympic Games.\nFinally, we explored three insights from our model and explain their reference value to country Olympic Committees.\n\nKeywords: Hierarchical Bayesian, Dirichlet Distribution, K-Means++, MLR, Penalized Lasso Regression, Dynamic Penalized Function, SA"
  },
  {
    "objectID": "MCM2025code.html#problem-background",
    "href": "MCM2025code.html#problem-background",
    "title": "The Riddle of Olympic Medal Table: Mathematical Model Prediction and Multi-factor Analysis",
    "section": "Problem Background",
    "text": "Problem Background\nAs one of the most influential comprehensive sports events in the world, the Summer Olympic Games will be held every four years, attracting the attention of people all over the world. Olympic medals are not only a platform for athletes to show themselves, challenge themselves and pursue excellence, but also reflect a country’s honor, sports strength and Economic strength and many other aspects. By modeling and forecasting the distribution of Olympic medals, it can better optimize the allocation of sports resources, stimulate the athletes’ competitive state and promote the development of science and technology sports."
  },
  {
    "objectID": "MCM2025code.html#restatement-of-the-problem",
    "href": "MCM2025code.html#restatement-of-the-problem",
    "title": "The Riddle of Olympic Medal Table: Mathematical Model Prediction and Multi-factor Analysis",
    "section": "Restatement of the Problem",
    "text": "Restatement of the Problem\nTo achieve these goals, we need to analyze the provided data and answer the following questions:\n\nDevelop a prediction model to forecast the 2028 Los Angeles Summer Olympics medal table and provide the medal distribution of countries compared to the 2024 Summer Olympics.\nBuild another prediction model to predict how many countries will win their first medal at the next Olympic Games. Model evaluation is also required.\nEstablish a model to analyze the relationship between the number and type of events in the Olympic Games and the number of medals won by each country, the important sports of different countries and the influence of the selection of home country events on the number of medals won.\nSelect three sports in which the country could benefit from a great coach and analyze the potential impact of the “great coach” effect on their country’s Olympic performance. Though the data of coach information here is not offered.\nExplore the original insights of the model and explain its help to the Olympic Committee.\n\n(Our thinking: The total number of medals is only relevant to time factors that reflected in our ARMA model, the ability of a country in each Olympics programs reflected only by the percentage of the number of medals in this program of this country / the total number of medals in this program, the chance reflected only by the percentage of a certain program / the total number of this program this year, the effect of being a host, the coach effect and other effects that needs to be observed but it is difficult without any other information.)"
  },
  {
    "objectID": "MCM2025code.html#assumptions",
    "href": "MCM2025code.html#assumptions",
    "title": "The Riddle of Olympic Medal Table: Mathematical Model Prediction and Multi-factor Analysis",
    "section": "Assumptions",
    "text": "Assumptions\n\nAll countries participating in the Paris 2024 Olympic Games will normally participate in the Los Angeles 2028 Olympic Games without withdrawing or refusing to participate for other reasons.\n\n\\(t\\) & Time series effect\n\n\\(\\beta_0\\) & Intercept\n\n\\(\\beta_1\\) & Host effect coefficient\n\\(\\gamma_j\\) & The impact coefficient of each project\n\\(\\lambda_i\\) & the total medal count\n\\(\\log(\\lambda_i)\\) & The logarithm of the expected number of medals\n$x_{ij} $ & \\(\\text{AthleteMedalRatio}_{ij} \\times \\text{ProjectPercent}_{j}\\)\n\\(\\alpha_i\\) & Time trend for each country (AR item).\n\\(x_1\\) & the host country\n\\(x_j\\) & the proportion of medals in an event\n\\(z_j\\) & item event ratio\n\\(x_j \\cdot z_j\\) & Interaction between medal ratio and event ratio\n\\(y_t\\)& quantified medal count in \\(t\\)year\n\\(\\beta_0\\) & the intercept term of the model\n\\(\\epsilon_t\\) & error term\n\n\\(R^2\\) & the coefficient of determination\nMAE & the Mean Absolute Error"
  },
  {
    "objectID": "MCM2025code.html#hierarchical-bayesian-composite-model",
    "href": "MCM2025code.html#hierarchical-bayesian-composite-model",
    "title": "The Riddle of Olympic Medal Table: Mathematical Model Prediction and Multi-factor Analysis",
    "section": "Hierarchical Bayesian Composite Model",
    "text": "Hierarchical Bayesian Composite Model\nWe first estimate the total number of gold medals, and then we model with negative binomial distribution, including OLS estimate \\(\\lambda\\) and Bayes formula +MCMC estimate \\(\\phi\\). After that, we use the Dirichlet distribution to estimate the gold, silver and copper ratio, and finally multiply the estimated total number of gold medals by the ratio to get the medal distribution.\n\nUsing OLS and ARMA to estimate \\(\\lambda_i\\)\n\nFirstly, we use OLS to estimate \\(\\lambda_i\\). Instead of selecting \\(\\lambda_i\\) directly, we use \\(\\log \\left(\\lambda_i\\right)\\). This is because we consider that logarithms compress the discreteness of the existing data, bringing it closer to a normal distribution. Besides, logarithmic transformation can better stabilize the variance, thereby improving the stability of the regression model and enhancing its prediction performance. The update equations of Hierarchical Bayesian are as follows: \\[\n\\log \\left(\\lambda_i\\right)=\\beta_0+\\beta_1 \\cdot \\operatorname{Host}_i+\\sum_{j=1}^M \\gamma_j \\cdot \\text { ProjectAbility }{ }_{i j}+\\alpha_i \\cdot t+\\epsilon_i\n\\]\nWhere: \\(\\alpha_i \\cdot t+\\epsilon_i \\sim \\mathcal{N}\\left(0, \\tau^2\\right)\\) denotes random error, same as in the ordinary regression residual term. Secondly, we further build time series model(AR and MA) to have a better estimate of the residual term \\(\\epsilon_i\\) by adding time-related information. 1. Autoregressive (AR) model:\nSuppose that the number of medals won by the country in this Olympic Games depends on its performance in the previous Olympic Games: \\[\ny_{i, t}=\\alpha_i \\cdot y_{i, t-1}+\\epsilon_{i t}\n\\]\nWhere \\(\\alpha_i\\) is the autoregressive coefficient. 2. Moving Average (MA) model:\nSuppose that the medal tally of the past few Olympic Games has an effect on the performance of this Olympic Games: \\[\ny_{i, t}=\\frac{1}{3} \\sum_{k=t-3}^{t-1} y_{i, k}+\\epsilon_{i t}\n\\] Team # 2509384 Page 9 of 26 3. Combined model of Autoregression and Moving Average:\nThe combination of autoregressive and moving average can be used to smooth out fluctuations in medal counts and capture long-term time trends in medal counts. The equation of ARMA model[2] used to describe the error term \\(\\epsilon_t\\) is as follows: \\[\n\\epsilon_t=\\phi_1 \\epsilon_{t-1}+\\phi_2 \\epsilon_{t-2}+\\cdots+\\phi_p \\epsilon_{t-p}+\\theta_1 \\eta_{t-1}+\\theta_2 \\eta_{t-2}+\\cdots+\\theta_q \\eta_{t-q}\n\\]\nBy substituting the error part \\(\\left(\\epsilon_t\\right)\\) of the ARMA model into the multi-level Bayesian model, we get the synthesized formula as follows: \\[\n\\begin{aligned}\n\\log \\left(\\lambda_i\\right)= & \\beta_0+\\beta_1 \\cdot \\text { Host }_i+\\sum_{j=1}^M \\gamma_j \\cdot \\text { ProjectAbility }_{i j}+\\alpha_i t+\\left(\\phi_1 \\epsilon_{t-1}+\\phi_2 \\epsilon_{t-2}+\\phi_3 \\epsilon_{t-3}+\\cdots\\right. \\\\\n& \\left.+\\phi_p \\epsilon_{t-p}+\\theta_1 \\eta_{t-1}+\\theta_2 \\eta_{t-2}+\\cdots+\\theta_q \\eta_{t-q}\\right)\n\\end{aligned}\n\\] - Using Hierarchical Bayesian and MCMC to estimate \\(\\phi\\)\nAccording to Bayes’ theorem, we have: \\[\nP(\\phi \\mid Y) \\sim P(Y \\mid \\phi) \\cdot P(\\phi)\n\\]\nWhere: - \\(P(\\phi \\mid Y)\\) is a posterior distribution. - \\(P(Y \\mid \\phi)\\) is the likelihood function. - \\(P(\\phi)\\) is the prior distribution.\nIn addition, we assume that \\(\\phi\\) follows the inverse gamma distribution[3]. \\[\n\\phi \\sim \\operatorname{InverseGamma}(\\alpha, \\beta)\n\\]\nThe probability density function of the inverse gamma distribution is: \\[\np\\left(\\phi^2\\right)=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\cdot\\left(\\phi^2\\right)^{-(\\alpha+1)} \\cdot \\exp \\left(-\\frac{\\beta}{\\phi^2}\\right)\n\\]\nThen, we use the Metropolis-Hastings algorithm for sampling. The basic idea of the Metropolis-Hastings algorithm is to sample from a complex posterior distribution by constructing a candidate distribution. Its formula for calculating the acceptance rate \\(\\alpha\\) for the given current sample \\(\\phi_{\\text {old }}^2\\) and candidate sample \\(\\phi_{\\text {new }}^2\\) is: \\[\n\\alpha=\\min \\left(1, \\frac{p\\left(y \\mid \\phi_{\\text {new }}^2\\right) \\cdot p\\left(\\phi_{\\text {new }}^2\\right)}{p\\left(y \\mid \\phi_{\\text {old }}^2\\right) \\cdot p\\left(\\phi_{\\text {old }}^2\\right)}\\right)\n\\]\nWhere, \\(p\\left(y \\mid \\phi^2\\right)\\) denotes the likelihood function. According to the calculation, we get a sample acceptance of \\(30 \\%\\).\n\n? Here we only use Year but not other information in the estimate in log\\(\\lambda\\) but in the MCM I just say it also adds all sports-related information…Maybe it is not suitable since we do not know future exact information about sport-related information. OMG. Also, time-series already gives us much information and if I add much more variables, maybe it will overfitting???\nBUt this model is also not very good\n\n# Install and load necessary libraries\nlibrary(rstan)\n\nLoading required package: StanHeaders\n\n\n\nrstan version 2.32.6 (Stan version 2.32.2)\n\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\nFor within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\nchange `threads_per_chain` option:\nrstan_options(threads_per_chain = 1)\n\nlibrary(forecast)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(bayesplot)\n\nThis is bayesplot version 1.11.1\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\n# Prepare your dataset (assuming 'data' is your dataframe)\ndata &lt;- read.csv(\"完美数据.csv\")\n\n# Filter out data for CHN (China) only\nchn_data &lt;- subset(data, NOC == \"CHN\")\n\n# 1. Estimate log(lambda) using OLS\nchn_data$log_total &lt;- log(chn_data$Total)\n\n# Fit OLS model for log(lambda)\nols_model &lt;- lm(log_total ~ Year, data = chn_data)\n\n# Extract the fitted values and residuals\nchn_data$fitted_log_lambda &lt;- fitted(ols_model)\nchn_data$residuals &lt;- residuals(ols_model)\n\n# 2. Estimate ARMA model for the residuals\narma_model &lt;- auto.arima(chn_data$residuals)\n\n# Calculate the residuals from the ARMA model\nchn_data$residuals_arma &lt;- residuals(arma_model)\n\n# 3. Estimate lambda for Negative Binomial\nlambda_estimates &lt;- exp(chn_data$fitted_log_lambda + chn_data$residuals_arma)\n\n# Prepare data for Stan (Negative Binomial model)\nstan_data &lt;- list(\n  N = nrow(chn_data),          # Number of data points\n  y = chn_data$Total,          # Total medals (observed data)\n  lambda = lambda_estimates    # Estimated lambda values\n)\n\n# Stan model code for Negative Binomial\nstan_code &lt;- \"\ndata {\n  int&lt;lower=0&gt; N;\n  int&lt;lower=0&gt; y[N];\n  real&lt;lower=0&gt; lambda[N];\n}\n\nparameters {\n  real&lt;lower=0&gt; alpha;  // Shape parameter of the Negative Binomial\n  real&lt;lower=0&gt; beta;   // Scale parameter of the Negative Binomial\n}\n\nmodel {\n  alpha ~ normal(0, 10);  // 设置alpha的先验分布\n  beta ~ normal(0, 10);   // 设置beta的先验分布\n  for (i in 1:N) {\n    y[i] ~ neg_binomial_2(lambda[i] * alpha, beta);\n  }\n}\n\"\n\n# Fit the model using Stan\nfit &lt;- stan(model_code = stan_code, data = stan_data, iter = 4000, chains = 4)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 6e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 4000 [  0%]  (Warmup)\nChain 1: Iteration:  400 / 4000 [ 10%]  (Warmup)\nChain 1: Iteration:  800 / 4000 [ 20%]  (Warmup)\nChain 1: Iteration: 1200 / 4000 [ 30%]  (Warmup)\nChain 1: Iteration: 1600 / 4000 [ 40%]  (Warmup)\nChain 1: Iteration: 2000 / 4000 [ 50%]  (Warmup)\nChain 1: Iteration: 2001 / 4000 [ 50%]  (Sampling)\nChain 1: Iteration: 2400 / 4000 [ 60%]  (Sampling)\nChain 1: Iteration: 2800 / 4000 [ 70%]  (Sampling)\nChain 1: Iteration: 3200 / 4000 [ 80%]  (Sampling)\nChain 1: Iteration: 3600 / 4000 [ 90%]  (Sampling)\nChain 1: Iteration: 4000 / 4000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.015 seconds (Warm-up)\nChain 1:                0.017 seconds (Sampling)\nChain 1:                0.032 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 4000 [  0%]  (Warmup)\nChain 2: Iteration:  400 / 4000 [ 10%]  (Warmup)\nChain 2: Iteration:  800 / 4000 [ 20%]  (Warmup)\nChain 2: Iteration: 1200 / 4000 [ 30%]  (Warmup)\nChain 2: Iteration: 1600 / 4000 [ 40%]  (Warmup)\nChain 2: Iteration: 2000 / 4000 [ 50%]  (Warmup)\nChain 2: Iteration: 2001 / 4000 [ 50%]  (Sampling)\nChain 2: Iteration: 2400 / 4000 [ 60%]  (Sampling)\nChain 2: Iteration: 2800 / 4000 [ 70%]  (Sampling)\nChain 2: Iteration: 3200 / 4000 [ 80%]  (Sampling)\nChain 2: Iteration: 3600 / 4000 [ 90%]  (Sampling)\nChain 2: Iteration: 4000 / 4000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.015 seconds (Warm-up)\nChain 2:                0.015 seconds (Sampling)\nChain 2:                0.03 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 4000 [  0%]  (Warmup)\nChain 3: Iteration:  400 / 4000 [ 10%]  (Warmup)\nChain 3: Iteration:  800 / 4000 [ 20%]  (Warmup)\nChain 3: Iteration: 1200 / 4000 [ 30%]  (Warmup)\nChain 3: Iteration: 1600 / 4000 [ 40%]  (Warmup)\nChain 3: Iteration: 2000 / 4000 [ 50%]  (Warmup)\nChain 3: Iteration: 2001 / 4000 [ 50%]  (Sampling)\nChain 3: Iteration: 2400 / 4000 [ 60%]  (Sampling)\nChain 3: Iteration: 2800 / 4000 [ 70%]  (Sampling)\nChain 3: Iteration: 3200 / 4000 [ 80%]  (Sampling)\nChain 3: Iteration: 3600 / 4000 [ 90%]  (Sampling)\nChain 3: Iteration: 4000 / 4000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.014 seconds (Warm-up)\nChain 3:                0.014 seconds (Sampling)\nChain 3:                0.028 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 4000 [  0%]  (Warmup)\nChain 4: Iteration:  400 / 4000 [ 10%]  (Warmup)\nChain 4: Iteration:  800 / 4000 [ 20%]  (Warmup)\nChain 4: Iteration: 1200 / 4000 [ 30%]  (Warmup)\nChain 4: Iteration: 1600 / 4000 [ 40%]  (Warmup)\nChain 4: Iteration: 2000 / 4000 [ 50%]  (Warmup)\nChain 4: Iteration: 2001 / 4000 [ 50%]  (Sampling)\nChain 4: Iteration: 2400 / 4000 [ 60%]  (Sampling)\nChain 4: Iteration: 2800 / 4000 [ 70%]  (Sampling)\nChain 4: Iteration: 3200 / 4000 [ 80%]  (Sampling)\nChain 4: Iteration: 3600 / 4000 [ 90%]  (Sampling)\nChain 4: Iteration: 4000 / 4000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.016 seconds (Warm-up)\nChain 4:                0.015 seconds (Sampling)\nChain 4:                0.031 seconds (Total)\nChain 4: \n\n# View summary of the model fitting\nprint(fit)\n\nInference for Stan model: anon_model.\n4 chains, each with iter=4000; warmup=2000; thin=1; \npost-warmup draws per chain=2000, total post-warmup draws=8000.\n\n        mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nalpha   1.01    0.00 0.08   0.86   0.95   1.01   1.06   1.18  4977    1\nbeta   20.87    0.09 6.62   9.18  16.11  20.43  25.06  34.96  5126    1\nlp__  -40.63    0.02 1.12 -43.59 -41.03 -40.28 -39.85 -39.56  2637    1\n\nSamples were drawn using NUTS(diag_e) at Sat May  3 18:33:06 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n# Extract posterior samples of alpha and beta\nposterior_samples &lt;- extract(fit)\n\n# 4. Predict for 2028 Olympics (CHN)\nfuture_years &lt;- data.frame(Year = 2028)\nfuture_lambda_log &lt;- predict(ols_model, newdata = future_years)\n\n# Ensure future_residuals_arma is numeric and extract the forecast values\nfuture_residuals_arma &lt;- as.numeric(predict(arma_model, n.ahead = 1)$pred)\n\n# Now we can safely add the values\nif (is.numeric(future_lambda_log) && is.numeric(future_residuals_arma)) {\n  future_lambda &lt;- exp(future_lambda_log + future_residuals_arma)\n} else {\n  stop(\"The predicted values are not numeric.\")\n}\n\n# Use posterior samples to generate predictions for the 2028 total medals count\nalpha_sample &lt;- posterior_samples$alpha\nbeta_sample &lt;- posterior_samples$beta\nfuture_medals_samples &lt;- rnbinom(length(alpha_sample), size = alpha_sample, mu = future_lambda)\n\n# Calculate the prediction for the 2028 total medals count (mean of posterior samples)\npredicted_medals &lt;- mean(future_medals_samples)\n\n# Output the predicted medals\npredicted_medals\n\n[1] 121.3319\n\n# Check pairs plot for sampling problems\nmcmc_pairs(fit)\n\n\n\n\n\n\n\n\n\n# Load necessary libraries for visualization\nlibrary(ggplot2)\n\n# Create a dataframe with the predicted medal samples\npredicted_medals_df &lt;- data.frame(predicted_medals_samples = future_medals_samples)\n\n# Create the visualization (Histogram with density curve)\nggplot(predicted_medals_df, aes(x = predicted_medals_samples)) +\n  geom_histogram(binwidth = 1, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  geom_density(aes(y = ..density..), fill = \"red\", alpha = 0.3) +\n  labs(title = \"Predicted Number of Medals for CHINA at the 2028 Olympics\",\n       x = \"Predicted Medals\",\n       y = \"Density\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12)\n  )\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\n4.2 Dirichlet Distribution\n\n4.2.1 Description of Dirichlet distribution\n\nModeling based on the scale of existing medals: For each country \\(i\\), we modeled the proportion of gold, silver and bronze medals it won in different events: \\[\np_i=\\operatorname{Dirichlet}\\left(\\alpha_i\\right)\n\\]\n\nWhere \\(p_i=\\left(p_{i 1}, p_{i 2}, p_{i 3}\\right)\\) represents the proportion of gold, silver and bronze medals won by a country’s \\(i\\) in different events, and \\(\\alpha_i\\) is used as the hyperparameter of Dirichlet distribution to reflect the contribution of different sports to the total number of medals of the country. 2. The number of medals in each event:Once the percentage and total number of medals for each country is obtained, we calculate the specific number of medals for each country in the gold, silver and bronze events using the following formula: \\[\ny_{i j}=p_{i j} \\cdot y_i\n\\]\nWhere \\(y_{i j}\\) represents the number of medals won by the country \\(i\\) in the event \\(j\\) (gold, silver or bronze), \\(p_{i j}\\) is the proportion of medals obtained by the Dirichlet distribution, and \\(y_i\\) represents the total number of medals won by the country \\(i\\). 4.2.2 Prior Distribution - Dirichlet distribution hyperparameter Settings : \\(\\alpha_{i j} \\sim \\operatorname{Gamma}(1,1)\\), representing a prior distribution of the proportion of medals for each country on the item \\(j\\). Team # 2509384 Page 11 of 26 - Time effect : For autoregressive coefficients and moving average coefficients, assume that their priors are normally distributed: \\[\n\\alpha_i \\sim \\mathcal{N}(0,10), \\quad \\beta \\sim \\mathcal{N}(0,1)\n\\] - Discrete parameter of negative binomial distribution : \\(\\phi \\sim \\operatorname{Gamma}(1,1)\\)."
  },
  {
    "objectID": "MCM2025code.html#k-means-model",
    "href": "MCM2025code.html#k-means-model",
    "title": "The Riddle of Olympic Medal Table: Mathematical Model Prediction and Multi-factor Analysis",
    "section": "K-Means++ Model",
    "text": "K-Means++ Model\nIn the upcoming Olympic Games, it is expected that several countries will win their first medals. To estimate this prediction, we used the K-means clustering method to predict which countries, that have never won a medal before, are most likely to win their first medal.\n5.1 K-means Clustering Steps\nThe K-means clustering algorithm follows these steps: 1. Choosing the \\(K\\) value: We determine how many clusters the data should be divided into, typically using methods like the elbow rule.\n\nCalculating distances: For each country, we calculate its position in the feature space and assign it to the nearest cluster based on its distance from the cluster center.\nUpdating the cluster centers: We recalculate the cluster center based on the data points in the cluster\nIterative optimization: Repeat steps 2 and 3 until the cluster centers converge or change very little.\n\nThe mathematical formula for calculating the distance between a data point and the cluster center is as follows: \\[\nd\\left(x_i, \\mu_k\\right)=\\sqrt{\\sum_{j=1}^n\\left(x_{i j}-\\mu_{k j}\\right)^2}\n\\] where: - \\(x_i\\) is the data point, - \\(\\mu_k\\) is the cluster center, - \\(n\\) is the number of features in the data.\nThrough this process, K-means can group countries based on their historical medal data and other relevant factors, helping to identify those with significant medal-winning potential in future Olympic Games.\n\nCredibility Score and Prediction Probability\nCredibility Score: Each country obtains a Credibility Score through analysis of clustering and capability data, with higher scores indicating a greater likelihood of winning medals. We can base predictions for first-time medals on the Credibility Score.\nProbability Calculation: For countries with higher Credibility Scores, we can define a threshold based on this score.\nProbability Distribution: Through the probability distribution, we can quantify these predictions. For example: \\[\nP(\\text { Medal })=\\frac{\\text { Credibility Score }}{\\text { Maximum Credibility Score }}\n\\] where \\(P\\) (Medal) represents the probability of a country winning its first medal, and the Credibility Score is normalized by the highest Credibility Score.\n\nThe prediction probabilities for each country’s likelihood of winning its first medal in the next Olympic Games are presented in the table below:"
  },
  {
    "objectID": "MCM2025code.html#use-sa-solving-target-function-of-the-improved-ratio-of-medals-per-year-with-penalized-dynamic-function-participating-in",
    "href": "MCM2025code.html#use-sa-solving-target-function-of-the-improved-ratio-of-medals-per-year-with-penalized-dynamic-function-participating-in",
    "title": "The Riddle of Olympic Medal Table: Mathematical Model Prediction and Multi-factor Analysis",
    "section": "Use SA solving target function of the improved ratio of medals per year with penalized dynamic function participating in",
    "text": "Use SA solving target function of the improved ratio of medals per year with penalized dynamic function participating in\nIn order to find out which projects in various countries need to hire “great coach” to help improve their performance, we established a Simulated Annealing(SA) algorithm to solve it.\n\n??Here, maybe it is imapproapriate to weight each independent variable using this correlation rule但是我不知道咋办了\nWe set up a formula to calculate the correlation. Based on the independent variables we presented in 7.1 section, we multiply them by different weights. For the quantified ability coefficient, we multiply it by the correlation between the medals won in two Olympic Games. For hosts or not, we multiplied their correlation by the difference in the number of medals between each of the last ten Olympic Games. For the weight of whether to use the coach, we are constructed by 1 minus the weight of the above two coefficients.\n\\(\\sum_{i=1}^{n} (\\text{Ability}_i \\cdot w_i + \\text{HostCountry}_i\\cdot w_2 + (1 - w_1 - w_2) \\cdot  x_i)\\)\n\\(\\alpha \\cdot \\max\\left(0, \\sum_{j=1}^{n} x_i - 5\\right)\\)\n–&gt; \\(\\sum_{i=1}^{n} (\\text{Ability}_i \\cdot w_i + \\text{HostCountry}_i\\cdot w_2 + (1 - w_1 - w_2) \\cdot  x_i) - \\alpha \\cdot \\max\\left(0, \\sum_{j=1}^{n} x_i - 5\\right)\\)\nThen we use 模拟退火解决这个\n每个国家最多5个项目教练效应，模拟退火告诉我们那些\\(x_i\\)是1然后知道哪些项目请教练\n\nresult\n\nThe medal increment was calculated using the simulated annealing model. Results show that the “celebrity coach effect” significantly boosts medals. Specifically, the United States sees the largest increase, especially in ice hockey and football, while Japan’s increment is 4, with a smaller effect due to its strong foundation in sports like gymnastics. Belarus has a modest increase of 2, as the coach effect is limited by a weaker foundation in these sports."
  },
  {
    "objectID": "MCM2025code.html#lasso-for-choosing-the-most-important-programs-for-each-country-但是如果按国家来看数据量不够啊",
    "href": "MCM2025code.html#lasso-for-choosing-the-most-important-programs-for-each-country-但是如果按国家来看数据量不够啊",
    "title": "The Riddle of Olympic Medal Table: Mathematical Model Prediction and Multi-factor Analysis",
    "section": "Lasso for choosing the most important programs for each country 但是如果按国家来看数据量不够啊？？",
    "text": "Lasso for choosing the most important programs for each country 但是如果按国家来看数据量不够啊？？\n\\[\\hat{\\beta} = \\text{argmin} \\left( \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\beta_1 x_1 - \\sum_{j=2}^{n} \\beta_j x_j - \\sum_{j=2}^{n} \\beta_j z_j - \\sum_{j=2}^{n} \\beta_{j+n} (x_j \\times z_j) \\right)^2 + \\lambda \\sum_{j=1}^{n} |\\beta_j| \\right)\\] \\(x_1\\) is Host(0,1)\n\\(X_j\\) is ability(medals/all medals)\n\\(Z_j\\) is chance(participanting program/all programs)\nThe j from 2 to n means the index of each program.\nAfter LASSO, we get penalized model which shirinkage some unimportant variables."
  },
  {
    "objectID": "MCM2025code.html#sensitive-analysis-of-bayesian-composite-model",
    "href": "MCM2025code.html#sensitive-analysis-of-bayesian-composite-model",
    "title": "The Riddle of Olympic Medal Table: Mathematical Model Prediction and Multi-factor Analysis",
    "section": "Sensitive analysis of Bayesian composite model",
    "text": "Sensitive analysis of Bayesian composite model"
  },
  {
    "objectID": "MCM2025code.html#一些处理数据的技巧希望下次掌握熟练一点",
    "href": "MCM2025code.html#一些处理数据的技巧希望下次掌握熟练一点",
    "title": "The Riddle of Olympic Medal Table: Mathematical Model Prediction and Multi-factor Analysis",
    "section": "一些处理数据的技巧–希望下次掌握熟练一点",
    "text": "一些处理数据的技巧–希望下次掌握熟练一点\n\n数据预处理\n当年某项sport某国的能力用medalpercetage表示 noc表示\n\n# 加载必需的库\nlibrary(dplyr)\nlibrary(tidyr)\n\n# 读取数据\n\n\n# 计算每个 NOC, Year 和 Sport 组合的奖项数量\nmedal_counts &lt;- athletes_data %&gt;%\n  group_by(NOC, Year, Sport, Medal) %&gt;%\n  summarise(Medal_Count = n(), .groups = 'drop') %&gt;%\n  spread(key = Medal, value = Medal_Count, fill = 0)  # 展开 Medal 列为各奖项类型\n\n# 计算每个 Year 和 Sport 组合的总行数\ntotal_counts &lt;- athletes_data %&gt;%\n  group_by(Year, Sport) %&gt;%\n  summarise(Total_Count = n(), .groups = 'drop')\n\n# 将 medal_counts 和 total_counts 合并\nmedal_with_total &lt;- medal_counts %&gt;%\n  left_join(total_counts, by = c(\"Year\", \"Sport\"))\n\n# 计算金、银、铜奖总数占总行数的比例\nmedal_with_total &lt;- medal_with_total %&gt;%\n  mutate(Total_Medals = Gold + Silver + Bronze,  # 包括 No medal 作为奖项\n         Medal_Percentage = Total_Medals / Total_Count)\n\n# 保留原始数据中的其他列并与奖项统计信息进行连接\nfinal_data &lt;- athletes_data %&gt;%\n  left_join(medal_with_total, by = c(\"NOC\", \"Year\", \"Sport\"))\n\n# 保存结果\nwrite.csv(final_data, \"aFina0.csv\", row.names = FALSE)\n\n# 查看前几行结果\nhead(final_data)\n\n\nnoc抓怒汉\n如果文件summerOly_medal_counts的Team列的每一行名在文件aFina0的同行有对应的NOC那么把这个文件的Team名变成NOC，如果没有对应的删除这个文件的一整行数据，生成新数据，其余列不变\n# 读取CSV文件\nsummerOly_medal_counts &lt;- read.csv(\"/Users/luyu/Desktop/MCM2025/new_summerOly_medal_counts_with_Host.csv\", stringsAsFactors = FALSE)\naFina0 &lt;- read.csv(\"aFina0.csv\", stringsAsFactors = FALSE)\n\n# 找到summerOly_medal_counts中Team列在aFina0的Team列中有对应值的行\nmatched_rows &lt;- summerOly_medal_counts$Team %in% aFina0$Team\n\n# 将匹配的行的Team列替换为对应的NOC\nsummerOly_medal_counts$Team[matched_rows] &lt;- aFina0$NOC[match(summerOly_medal_counts$Team[matched_rows], aFina0$Team)]\n\n# 删除没有匹配的行\nsummerOly_medal_counts &lt;- summerOly_medal_counts[matched_rows, ]\n\n# 保存新的数据到CSV文件\nwrite.csv(summerOly_medal_counts, \"new_summerOly_medal_counts.csv\", row.names = FALSE)\n\n\n唯一标识组是Noc 和Year 统计参加每种Sport的数量 合成一个新的csv 每个Sport弄成单独一列数值是Noc与Year这行参加的数量\n# Load required library\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Load the data\ndata1 &lt;- read.csv(\"aFina0.csv\")\n\n# Step 1: Group by NOC and Year, and count the number of occurrences for each Sport\nsport_count &lt;- data1 %&gt;%\n  group_by(NOC, Year, Sport) %&gt;%\n  summarise(Sport_Count = n(), .groups = 'drop')\n\n# Step 2: Spread the data so that each Sport is a column\nsport_count_spread &lt;- sport_count %&gt;%\n  spread(key = Sport, value = Sport_Count, fill = 0)\n\n# Step 3: Save the result to a new CSV file\nwrite.csv(sport_count_spread, \"sport_count_by_NOC_Year.csv\", row.names = FALSE)\n\n# Optionally, view the result\nhead(sport_count_spread)\n\n\n如果NOC和Year一样，把"
  }
]